{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Basics\n",
    "\n",
    "## Using NLTK and Scikit-learn\n",
    "\n",
    "\n",
    "\n",
    "The Natural Languagae Tool-Kit (NLTK) contains many functions that help you understand Natural Language Processing \n",
    "\n",
    "There are many other libraries out there that have many tools and functions to improve you NLP model, but the concepts are the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "I'll walk through each step - data preprocessing, creating the corpus, making the sparse matrix, then finally running the regression\n",
    "The data I'm using is a tsv (tab-separated) file containing 1000 reviews of a restaurant. \n",
    "\n",
    "\n",
    "There are two columns:\n",
    "\n",
    "1) The text of the review\n",
    "2) 1, if the review is positive, 0, if the review was negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\parth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 1 Importing the required libraries:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# This data is tab-separated, instead of comma-separated because one of the columnd contains \n",
    "# text which may have commas -- confusing the pandas library into making more columns than there are\n",
    "\n",
    "df = pd.read_csv('Restaurant_Reviews.tsv', delimiter='\\t', quoting = 3)\n",
    "\n",
    "# We'll use the Regular Expression library to filter the text and the NLTK library to tokenize and stem the words\n",
    "# I'll explain the use of each library in underneath\n",
    "import re\n",
    "\n",
    "# This step is necessary only if this is the first time you're using the nltk library\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expressions:\n",
    "\n",
    "A review may contain punctuations, emoticons, emojis, or numbers. When trying to analyze the sentiment in the review, these characters don't help the algorithm understand if the reviewis positive or negative\n",
    "\n",
    "#### StopWords:\n",
    "\n",
    "StepWords is the list of words name the NLTK library has given to prepositions, articles, conjuctions etc. that are necessary for a sentence structure, but can skew the NLP model. If these words are left in the model, it'll make it think those words are contributing to the positive or negative review -- which is not the case.\n",
    "\n",
    "#### Stemming:\n",
    "\n",
    "The PorterStem function finds the root of each word. For example, words like 'Loved' become 'love' etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Wow... Loved this place.\n",
      "Step 2:Wow    Loved this place \n",
      "Step 3: wow    loved this place \n",
      "Step 4: ['wow', 'loved', 'this', 'place']\n",
      "Cleaned review: wow love place\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Cleaning the input to a suitable format\n",
    "# First we'll clean one review as an example:\n",
    "\n",
    "review = df['Review'][0]\n",
    "print('Step 1: '+review)\n",
    "\n",
    "\n",
    "review = re.sub('[^a-zA-Z]', ' ',  review)\n",
    "print('Step 2:'+review)\n",
    "\n",
    "review = review.lower()\n",
    "print('Step 3: '+review)\n",
    "# This separates each word into an indivual string and puts them all in a list:\n",
    "review = review.split()\n",
    "\n",
    "print('Step 4: '+str(review))\n",
    "\n",
    "# This line takes each word in the above list, finds that word's stem, and verifies that it isn't in the list of StopWords\n",
    "# And returns the words as a list:\n",
    "ps = PorterStemmer()\n",
    "review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "\n",
    "# Now that we have the required words, we join them together as a string:\n",
    "review = ' '.join(review)\n",
    "print('Cleaned review: '+review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Now we'll do this for all reviews:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "# There are 1000 reviews in this dataset\n",
    "for i in range(1000):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df['Review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the Learning step:\n",
    "\n",
    "The first step of the Learning process is to create the Count Vectorizer matrix. \n",
    "The CV Matrix is a 2D array with oen row for each review, and one column for each unique word in all reviews.\n",
    "\n",
    "The value of each column for a review is either 1 or 0.\n",
    "\n",
    "1, if that word is in that review, 0, if not. As a result most columns for all rows have the value 0.\n",
    "\n",
    "We can see what I mean by this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Count Vectorizer Matrix: (1000, 1565)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1555</th>\n",
       "      <th>1556</th>\n",
       "      <th>1557</th>\n",
       "      <th>1558</th>\n",
       "      <th>1559</th>\n",
       "      <th>1560</th>\n",
       "      <th>1561</th>\n",
       "      <th>1562</th>\n",
       "      <th>1563</th>\n",
       "      <th>1564</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1565 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...   1555  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...      0   \n",
       "\n",
       "   1556  1557  1558  1559  1560  1561  1562  1563  1564  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 1565 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "check_shape = np.array(X)\n",
    "print('Shape of the Count Vectorizer Matrix: ' + str(check_shape.shape))\n",
    "print('\\n\\n')\n",
    "shape_df = pd.DataFrame(check_shape)\n",
    "shape_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the shape:\n",
    "\n",
    "The array created by the Count Vectorizer has 1000 rows, one for each review. \n",
    "And 1565 columns, as there are 1565 unique words in all the reviews that are not in the StopWords list.\n",
    "\n",
    "And as you can see, most of the column values in the dataframe are 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer accepts an argument called 'max_features' that limits the number of unique words to be added in the Matrix\n",
    "This agrument only makes columns of the n most frequent words used:\n",
    "\n",
    "\n",
    "There are other arguments for the CountVectorizer function that you should explore:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the outout vector:\n",
    "\n",
    "y = df.iloc[:, 1].values\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "# Setting 80% Training, 20% testing set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Intuition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79 18]\n",
      " [39 64]]\n",
      "\n",
      "\n",
      "Accuray Rate = 0.715\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', max_features = 1000, random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy_rate = (cm[0][0]+cm[1][1])/(cm[0][1]+cm[1][0]+cm[0][0]+cm[1][1])\n",
    "print(cm)\n",
    "print('\\n')\n",
    "print(\"Accuray Rate = \" + str(accuracy_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the confusion matrix\n",
    "\n",
    "The confusion matrix is a 2X2 matrix that stores the number:\n",
    "\n",
    "'''\n",
    "[\n",
    "\n",
    "    [<'1' Label predicted as '1'>, <'1' Label predicted as '0'>],\n",
    "    \n",
    "    [<'0' Label predicted as '1'>, <'0' Lable predicted as '0'>]\n",
    "                                    \n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "Thus, the error rate is calclated as the ratio of correct predictions to total predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The accuracy rate we go was a little over 71%, which is not great. But this is where we get to experiment.\n",
    "We can could test out a variety of hyperparameters so that the accuracy improves, but that might lead to overfitting.\n",
    "\n",
    "This is where the field of Machine Learning becomes more of an art than science. You get to play around with the number of samples to train on, choosing the number of trees in the Random Forest Classifier, choosing the maximum number of features in the CountVectorizer, the Test/Train split of the dataset. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
